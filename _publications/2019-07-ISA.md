---
title: "Interlaced Sparse Self-Attention for Semantic Segmentation"
collection: publications
permalink: /publication/2019-07-ISA.md
excerpt: 'This paper proposed an efficient self-attention.'
date: 2019-07
venue: 'arXiv:1907.12273 | IJCV2021'
paperurl: 'https://arxiv.org/abs/1907.12273'
citation: 'Lang Huang, Yuhui Yuan, Jianyuan Guo, Chao Zhang, Xilin Chen, Jingdong Wang (2019). &quot;Interlaced Sparse Self-Attention for Semantic Segmentation; <i>arXiv:1907.12273</i>.'
---
This paper aimed to improve the efficiency of the self-attention on high resolution inputs. We proposed the Interlaced Sparse Self-Attention (ISA) that decomposes the pairwise attention matrix into two sparse block-diagonal matrices, which reduces the complexity from $\mathcal{O}(\mathrm{N}^2)$ to $\mathcal{O}(\mathrm{N}^{1.5})$. The core idea of ISA has been applied in many Vision Transformer backbones, including Shuffle Transformer, Twins Transformer, and MaxViT.

[Paper](https://arxiv.org/pdf/1907.12273)|[Code](https://github.com/openseg-group/openseg.pytorch)|[Zhihu (in Chinese)](https://zhuanlan.zhihu.com/p/557738335)